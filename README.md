Poem generating was achieved using 2 models, the Markov chain and Bayesian network using: https://www.kaggle.com/datasets/paultimothymooney/poetry

Markov Chain:

Unlike a Bayesian network, the code focuses on Markov chain modelling, where word transitions are probabilistically determined based on the order of preceding words.

Global Variables:
order: Determines the number of preceding words considered for predicting the next word Markov chain.
word_probabilities: A dictionary to store the probabilities of each word following a specific set of preceding words.

Functions:
markov(dataset_folder): Trains the Markov chain on a dataset of poems by analyzing word transitions. Its parameter is dataset_folder, which is the path to the folder containing .txt files with poems.
generate(start, length): Generates a poem based on the trained Markov chain using a specified starting word. Its parameters are start, which is the word from which the poem generation begins, and length, which is the desired length of the generated poem in words.

Training Process:
1.	The word_probabilties is to ensure a fresh start for each training session.
2.	A list of file paths made in the specified dataset_folder.
3.	Each .txt fileâ€™s content is read.
4.	Separate poems assuming the poems in the dataset are separated by a single character ("-").
5.	Poems are split into lines.
6.	Extracts words from each line, skipping lines with fewer words than the specified order.
7.	Build a Markov chain by updating the word_probabilities dictionary based on word transitions.

Generating a Poem:
A poem is generated by calling generate(start, length) with a starting word and the desired length of the poem. The generated poem is printed with line breaks and formatted to a specified width. Probabilities of word transitions are printed for each word in the generated poem.


Bayesian Network:

A Bayesian Network is used to capture probabilistic relationships between consecutive words in a set of poems. It then generates new poems based on the learned transition probabilities.

Additional Dependencies:
pgmpy.models.BayesianModel is a library used for constructing and representing Bayesian Networks.

Functions:
bayesianNetwork(poems, n_gram=2) constructs a Bayesian Network model based on word transitions in a set of poems. Its parameters are poems, which is a list of poems represented as text, and n_gram, which is the order of the model, determining the number of preceding words considered for predicting the next word. Defaults to 2.
generate(bayesian_network, words=50, start=('', '')) generates a poem using a Bayesian Network model. Its parameters are bayesian_network, which is a Bayesian Network model constructed by bayesianNetwork, words, which is the desired length of the generated poem in words. Defaults to 50, and start, the starting word for the poem in the form of a tuple. Defaults to an empty tuple.

Building the Bayesian Network:
The Bayesian Network is constructed using the bayesianNetwork function. It analyzes the transitions between consecutive words in a set of poems, building a Bayesian Network model representing these probabilistic relationships.

Training Process:
1.	Split each poem into individual words or tokens.
2.	Choose the order of the N-gram, which determines the number of preceding words considered for predicting the next word.
3.	Create a dictionary to store transition probabilities between consecutive N-grams.
4.	Count the occurrences of transitions between N-grams and the following word.
5.	Use the collected transition counts to build a Bayesian Network model, where nodes represent N-grams, and edges represent transitions.
6.	The model captures the probabilities of transitioning from one N-gram to another based on the observed occurrences in the training dataset.
This is made for all the text files (poems).

Generating a Poem:
To generate a poem, the user is prompted to input the first two words. The generate function uses the constructed Bayesian Network to probabilistically select following words.
